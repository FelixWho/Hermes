yeah yeah i mean if we just want to have um some data for the user face could even be random data uh mm mm yeah i'm hmm yes hmm yes hmm i'm not so sure i i thought we would just have like um one big summary um with all the uh different importance levels um displayed on it and depending on what our um zoom level is we just display a part of it and we would have one very big thing off-line and from that we would just select what we are displaying yes so for example you would um give a high value to those um sequences you want to display in the meeting series summary and you just cut off that was what i sh i thought yeah i thought but i think the m difference might be that we want just want to have um the words and that's not so much what he meant with not possibly loading everything was that you m um load all the uh annotation stuff all the sound files all in um i r i i'm getting quite lost um at the moment because um w what's um our difference between the um se um uh the importance measure and the skimming i mean do we do both or is it the same thing okay so but when when we talk about summaries you talk about this uh abo about skimming and not about yeah yeah right isn't that the skimming isn't that the skimming yeah but it use the same data yeah a and yeah i think we also thought about combining that measure with um the measures i get from um s uh hot-spots and so on so that would also be on utterance level i think i think yes sure yes yes right oops it does so i define baseline and what it loads for example it loads all the utterances and so on but it doesn't load um the discourse acts and for example not the and what's what else there not the summaries it only loads those on demand y you mean that you um basically split up th the big thing into um different summaries for example that you have a very um top-level um summary and a separate file for for each level mm-hmm yes n uh no no it's f for no you're right yeah it's for um no i i think we would just take the segments that are already that were yeah there's um this segments file um you know the x_m_l_ segments oh that i don't know yeah that's um mm-hmm there there are time stamps um for well segments um and for th um segments is for example when when you look at the data what is displayed in one line what when when you look at it in the hmm i think so isn't um for ex um i i compared it with what i did for the pause um duration extraction um and basically it's uh words that are uttered in a sequence without pauses but sometimes um however there are um short pauses in it and they're indicated by square brackets pause or something in the data um someti uh but uh the annotators decided what was one segment and what wasn't i think so yeah but um i think for some annotations um an uttera ca utterance can have several um types for example for the dialogue acts and so on okay yeah that should be for yeah should be yeah yes but that's yeah everything that's a word has a sti time stamp that's at the end that's at the end i think her time yeah maybe didn't have a look at our meetings uh i i think it wouldn't as it occurs i mean it would be it occurs in every meeting so and i think it even has uh its own annotation like digits or something so that should be really easy to cut out yeah i'm sure ah it's just to test the system i think so mm they have to read numbers from uh i didn't have a look at that so they mm-hmm uh th yeah 'kay um i just um wondered so who's uh then doing um the frequencies on on the words because i'm i think i will also um i could also make use of it um for the agreement and disagreement thing because i um i in my outline i talked about um using the um discourse acts first and um then in the chunks of text i found looking for word patterns and so on so um i would for example need the um most freq um frequent words so if you cut off all that i'd won't be use or yeah i i but i need it for my chunks then i would you know yeah but i'd uh i would like to look at the frequency of words in my um in the regions of text i found out to be interesting so i wouldn't need it it it would have to be re-calculated only for my segments huh uh uh mm i think it would be you know l as as big at as the hot-spot annotation things that's quite small yeah that's some utterances yes yeah yeah so i would probably just concatenate all my um text chunks and then let's say m i will run over it yes yes definitely yeah right ye m um jasmine uh um what is um the text you're extracting uh looking like then at the end because um i i think it's actually very similar to what i did for my um speaker um uh extraction and i think i would ch perhaps have to change two lines of codes to get you um for each meeting a file that says fr from um this millisecond to this millisecond there was this sequence of words and so on so that's just changing two lines of code and it would give you that so um yeah so far i extracted um the dura durations but it's from the words file so i could just um contatenate concatenate um the words instead of the durations and it should i mean should be very straight-forward i can try to do it and send it to you pe and you have a look at it will it make sense for what you want yeah uh p i mean it i just let it run over all the files so yes i just ordered uh i ordered according to the um starting times of the utterances what do you mean by diffe yeah i mean t i i have one what i give you would be one file for each meeting yeah not for each meeting series i didn't do that yet yeah one group yeah yeah i mean there's one series that has just one meeting yes um the you you the data is of the form you have um three identification letter so b_e_d_ or b_b_d_ or something and that's always the same group and then after that there's um a number like o_o_ one o_o_ two so it's a yeah but that's that's really quite easy to see because they're named yes but i i mean as um the start uh start times um start for each meeting at zero you could just probably just um add the um final second time to the next meeting and so on and just put it all together but then we would have to change um the information about who on which channel it was set um to by which person it was set and that is actually stored in another x_m_l_ document yeah i w would then just not print out the um start and end times no it's for every single word or for every single utterance yeah that depends on what you want yeah but i do it with perl it's just string manipulation so i would i mean i would just sure no i didn't do a sea no and you would want that all in one file for all the corpus or for the series yeah i can directly put it into uh just like so uh only words um per meeting series uh-huh yes yeah they will just i will just take i would uh take over the names they have anyway yeah yeah yeah one series has the um same three starting letters so so only words and words and times and you yeah you want it ordered okay okay anybody um ord base dot times yeah and do you want yeah sometimes they're contained in one another so just after th mm-hmm 'kay ordered only words um and i think um for all the corpus it's just from i know from other times it's um nine megami byte to have i mean should be should be similar to have the words should be na um all the words together um for all the meetings that's what i'm guessing that's you know um what i because nine mega-byte is what i got for when i said for every um utterance this is goes from there to there and takes takes seconds oh yeah i mean i'm it doing it for all of it doesn't matter yeah i mean i hope it will be the same for the words it's just what i i mm-hmm mm so so um i will probably send um just one file of the first meeting um to all those who need it so that you can have a look whether that's what you want yeah i mean if it's just for one meeting it's really not too big yeah what do we have to demonstrate