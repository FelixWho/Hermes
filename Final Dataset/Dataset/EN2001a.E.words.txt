'kay gosh 'kay is there much more in it than he d is there much more in it than he said yesterday mm hmm hmm yeah now i'd say if for the prototype if we just like wherever possible p chunk in the stuff that we have um pre-annotated and stuff and for the stuff that we don't have pre-annotated write like a stupid baseline then we should probably be able to basically that means we focus on on the interface first sort of so that we we take the the ready-made parts and just see how we get them work together in the interface the way we want and and then we have a working prototype and then we can go back and replace pieces either by our own components or by more sophisticated compo po components of our own so it's probably feasible the thing is i'm away this weekend so that's for me oh yeah um yeah no but also i might like the the similarity thing like my just my matrix itself for my stuff i c i i think i can do that fairly quickly because i have the algorithms yeah i think today's meeting is really the one where we where we sort of settle down the data structure and as soon as we have that uh probably like after today's meeting we then actually need to well go back first of all and look at nite x_m_l_ to see in how far that that which we want is compatible with that which nite x_m_l_ offers us and then just sort of everyone make sure everyone understand the interface so i think if today we decide on what data we wanna have now and and later maybe even today we go and look at nite x_m_l_ or some of us look at nite x_m_l_ in a bit more detail just trying to make some sense of that code and see how does the representation work in their system and then sort of with that knowledge we should be able to then say okay that type of nite x_m_l_ data we wanna load into it and this is how everyone can access it and then we should be able to go from there no i've looked looked at the documentation and n like seen enough to make me think that we want to use the nite x_m_l_ framework because um they have a good a event model that synchronizes sort of the data and and every display element so that takes a lot of work away from us sort of that would be a reason for staying within their framework and using their general classes but beyond that i haven't looked at it at all which is something we should really do who actually like for this whole discussion i mean who of us is doing stuff that is happening on-line and who of us is doing stuff that's happening off-line like my data is coming c hmm yeah okay okay 'kay so basically apart from the display module the i the display itself we don't have an extremely high degree of interaction between sort of our modules that create the stuff and and the interface so the interface is mainly while it's running just working on data that's just loaded from a file i guess there isn't yeah i know th yeah the search is i guess the search is sort of a strange beast anyway because for the search we're leaving the nite x_m_l_ framework um but that's still sort of that's good that means that at least like we don't have the type of situation where somebody has to do like a billion calculations on on data on-line 'cause that would make it a lot more like that would mean that our interface for the data would have to be a lot more careful about how it performs and and everything and nobody is modifying that data at at on-line time at all it seems nobody's making any changes to the actual data on-line so that's actually making it a lot easier that basically means our browser really is a viewer mostly which isn't doing much with the data except for sort of selecting a piece piece of it and and displaying it hmm well some parts relevant for the search yes i'd say so hmm yeah but nobody of us is doing much of searching from the data in the on-line stage and for all together like the display itself i think we are easier if we if it's sitting on the x_m_l_ than if it's sitting on the s_q_l_ stuff because if it's sitting on the x_m_l_ we have the the nite x_m_l_ framework with all its functionality for synchronizing through all the different levels whenever there's a change whenever something's moving forward and stuff and we can just more or less look at their code like how their player moves forward and how that moving forward is represented in different windows and stuff so i think in the actual browser itself i don't wanna sit on the s_q_l_ if we can sit on the x_m_l_ because sitting on the x_m_l_ we have all we have so much help and for y for like the p the calculations that we're doing apart from the search it seems that everyone needs some special representations anyway you mean our results yeah in in the nite x_m_l_ x_m_l_ format so with their time stamps and stuff so that it's easy to to tie together st things what i'm like what we have to think about is if we go with this multi-level idea like this idea that sort of if you start with a whole meeting series as one entity as one thing that you display as one whole sort of that then the individual chunks of the individual meetings whereas and then you can click on a meeting and then sort of the meeting is the whole thing and the chunks are the individual segments that means sort of we have multiple levels of of representation which we probably if we if we do it this way like we f we have to discuss that if we do it this way then we should probably find some abstraction model so that the interface in the sense like deals with it as if it's same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting you know what i mean and that's probably stuff that we have to sort of like process twice then like for example that like the summary of a meeting within the whole meeting corpus or meeting series y is meeting series a good word for that i don't really know what how to call it you know what i mean like not not the whole corpus but every meeting that has to do with one topic um so in in the meeting se series so that a summary for a meeting within the meeting series are sort of compiled off-line by a summary module and that is separate from a summary of a segment within a meeting 'cause i don't think we can so are we doing that at all levels are we um and just have different like fine-grainedness levels sort of mm 'kay so the only thing that yeah so the only thing that would happen basically if i double-click let's say from the whole meeting series on a single meeting is that the zoom level changes like the th the start and the end position changes and the zoom level changes i i thought we couldn't do that like i was under the impression that we couldn't do that because we couldn't load the data for all that but i don't know i mean that so i'm s not sure if i got it i was mm-hmm mm-hmm mm-hmm mm-hmm okay so okay i wa i was just worried about the total memory complexity of it but i i completely admit i mean i just sort of like th took that from some thing that jonathan once said about not loading everything but maybe i was just wrong about it how many utterances w yeah and i w yeah yeah yeah yeah so what we have is we would have a word like we would have words with some priority levels and they would basically be because even the selection would would the summaries automatically feed from just how prioritized an individual word or how indiv uh prioritized an individual utterance is or i are the summaries sort of refined from it and made by a machine to make sentences and stuff or are they just sort of taking out the words with the highest priority and then the words of the second highest priority and the u okay are we doing it on th the whole thing on the utterance level or are we doing it on word level like the information density calculation we i think we have start and end times for words actually but it's yeah but it m it might s but it might sound crazy in the player we should really maybe we can do that together at some point today that we check out how the player works but there's maybe some merit in altogether doing it on an utterance level in the end so yeah well but also about the displays i mean the displays in the in the text body in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance for utterance but it's also displaying uh a summarised version in you know like below the below the graph the part maybe yeah r hmm oh yeah f it's just like there there's like audio skimming and there's displayed skimming yeah ma maybe there's some merit of going altogether for utterance level and not even bother to calculate i mean if you have to do it internally then you can do it but maybe like not even store the importance levels for individual words and just sort of rank utterances as a whole hmm yeah 'cause it it might be better skimming and less memory required at the same time and i mean if you if you know how to do it for individual words then you can just in the worst case if you can't find anything else just sort of make the mean of the words over the utterance you know what i mean w it's it's well what's the smallest chunk at the moment you're thinking of of assigning an importance measure to is it a word or is it an utterance so we're thinking of like maybe just storing it on a per utterance level because it's it's less stuff to store probably for dave in the in the audio playing and for in the display it's probably better if you have whole utterances than i don't know like what it's like if you just take single words out of utterances that probably doesn't make any sense at all whereas if you just uh show important utterances but the utterance as a whole it makes more sense so it doesn't actually make a difference for your algorithm 'cause it just means that if you're working on a word level then we just mean it over the utterance they are on oh so that's good anyway then yeah because that makes it a lot easier than to t put it on utterance level oh yeah no but i mean like how how jasmine does it internally i don't know but it's probably yeah you probably have to work on word levels for importance but there should be ways of easily going from a word level to an utterance level okay yeah prob hmm well we do a pre-filtering of sort of the whole thing sort of like but that like the problem with that is it's easy to do in the text level but that would mean it would still play the uh in your audio unless we sort of also store what pieces we cut out for the audio yeah i think before we can like answer that specific question how we c deal with that it's probably good for us to look at what the audio player is capable of doing yes so what do you mean by buffering like you think directly feeding but yeah but not but not stored on the hard disk and then loaded in but loaded in directly from memory but it's probably a stream if it exists in java it would be probably some binary stream going in of some type okay yeah okay okay so i mean so that means that there's probably even if you go on an per utterance level there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all and that maybe also for the audio we'd have to do so let's say we play the whole au phrase but then in addition to that we have some information that says minus that part of something that's okay that we can do yeah maybe even i mean that's sort of that depends on how how advanced we get if maybe if we realise that there's massive differences in in gain or in something you can probably just make some simple simple normalization but that really depends on how much time we have and and how much is necessary yeah if like i d i don't know anything about audio and i have never seen the player so if you find that the player accepts some n input from memory and if it's easy to do then i guess that's that's fairly doable so but that means in the general structure we're actually quite lucky so we we have we load into memory for the whole series of meetings just the utterances and rankings for the utterances and some information probably that says well the i guess that goes with the utterance who's speaking because then we can also do the display about who's speaking yeah but i'm i'm still confused 'cause i thought like that's just what jonathan said we do c that we can't do like load a massive document of that size on the other hand the other hand i mean it shouldn't be like should be like fifty mega-byte in ram or something it shouldn't be massive should it actually fifty hundred megabyte is quite big in ram just thinking what's the simp so we do get an error message with the project if we load everything into the project with all the data they load so we know that doesn't work so our hope is essentially that we load less into it what's this lazy loading thing somebody explain lazy loading to me ah okay so that is that only by type of file like if if if the same thing is in different files would it then maybe like you know if if utterances are split over three or ten or w hundred different files is then a chance maybe that it doesn't try to load them all into memory at the same time but just so why does it fail then in the first place then it shouldn't ever fail because then it should never yeah but yeah but um it uh it it failed right when you load it right the nite x_m_l_ kit so that's interesting hmm let's check that out um i'll p i'll probably ask jonathan about it so alternatively if we realise we can't do the whole thing in one go we can probably just process some sort of meta-data you know what i mean like sort of sort of for the whole series chunks representing the individual meetings or some like something that represents the whole series in in a v in a structure very similar to the structure in which we represent individual um meetings but with data sort of always combined from the whole series so instead of having an single utterance that we display it would probably be like that would be representing a whole um topic a segment in a meeting and sort of so that wi using the same data st well in a sense uh i'm i'm thinking of in a sense of like creating a virtual a virtual meeting out of the whole meeting series sort of yeah sort of like off-line create a virtual meeting which which basically treats the meeting series as if it was a meeting and treats the individual meetings within the series as if they were segments and treats the individual segments within meetings as if they were um utterances you know so we just sort of we shift it one level up and in that way we could probably use the same algorithm and just like make vir like one or two ifs that say okay if you are on a whole document uh a whole series level and that was a double-click then don't just go into that um segment but load a new file or something like it but in general use the same algorithm that would be an alternative if we can't actually load the whole thing and 'cause also like even if we maybe this whole like maybe i'm worrying too much about the whole series in one thing display because actually i mean probably users wouldn't view that one too often yeah but i'm i'm still worried like for example for the display if you actually if you want a display uh like for the whole series the information density levels based on and and the f and the only granularity you have is individual utterances that means you have to through every single utterance in a series of seventy hours of meetings yeah yeah and if you make that structurally very similar to the the le like one level down like the way how we uh store individual utterances and stuff then maybe we can more or less use the same code and just make a few ifs and stuff yeah so so but still so in in general we're having we're having utterances and they have a score and that's as much as we really need and of cou and they also have a time a time information of course hmm and a and a s and a speaker information yeah yeah so an information which topic they're in yeah and and probably separate to that an information about the different topics like that yeah so so the skimming can work on that because the skimming just sort of sorts the utterances and puts as many in as it needs yeah yeah it'll it'll play them in some order in which they were set because otherwise it's gonna be more entertaining um but that that's enough data for the skimming and the the searching so what the searching does is the searching leaves the whole framework goes to the s_q_l_ database and gets like basically in the end gets just a time marker for where that is like that utterance that we are concerned with and then we have to find i'm sure there's some way in in nite x_m_l_ to just say set position to that time mark and then it shifts the whole frame and it alerts every single element of the display and the display updates yeah yeah that we can ju yeah but so so if if somethi so yeah so if in that tree display somebody clicks on something yeah and then you sort of feed the time stamp to and the nite x_m_l_ central manager and that central manager alerts everything that's there like alerts the skim like the the audio display alerts the text display alerts the visual display and says we have a new time frame and then they all sort of do their update routines with respect to the current level of zoom so how much do they display and starting position at where the or maybe the mid-position of it i don't know like w if start where the thing was found or if that thing wa was found it's in the middle of the part that we display that i don't know but that we can decide about but a general sort of it's the same thing if like whether you play and it moves forward or whether you jump to a position through search it's essentially for all the window handling it's the same event it's only that the event gets triggered by the search routine which sort of push that into nite x_m_l_ and says please go there now why do we have to do it in memory but that stuff's so i mean like the information is coming from off-line so we probably we don't even have to change the utterance document right because the whole way like the whole beauty of the nite x_m_l_ is that it ties together lots of different files so we can just create an additional x_m_l_ file which for every utterance like the utterances have i_d_s i presume some references so we just we tie uh p just a very short x_m_l_ file which it's the only information it has that has whatever a number for for the um weight for the information density and we just tie that to the existing utterances and tie them to the existing speaker changes well otherwise we probably have to go over it and like add some integer that we just increment from top to bottom sort of to every utterance as an as an i_d_ some type or un or try to understand how nite x_m_l_ i_d_s work and maybe there's some special route we have to follow when we use these i_d_s it's alm hmm yeah the the girl said the utterances themselves are not numbered at the moment okay okay okay yeah so i guess that would be solvable if not mm-hmm sorry okay okay is that a board marker pen actually oh that's just so like to make a list of all this stuff or we probably can somebody can do it on paper all these fancy pens so what so the stuff we have we have utterances and speakers and weights for utterances so for for every utterance sort of like the utterance has a speaker and a weight which is coming from outside or we just tie it to it and there is segments which hmm oh so sorry um uh topic s topic segments i meant like they are they are a super-unit so so the utterances are tied to topic segments and if the time stamps are on a word level then we b somehow have to extract time stamps for utterances where they start w what segments now okay is the uh is that the same as utterances that is that the same as utterances that mm-hmm mm-hmm what so that's oh but that's one o one segment or is that two segments then yeah okay okay so but but generally utterances is that which we just called uh sorry segments is that which we just called utterances now like it's it's the sa it's sort of like one person's contribution at a time sort of thingy dingy okay so yeah so we have those and and then we have some f field somewhere else which has topics yeah and and a topic's basically they are just on the i_d_ probably with a start time or something and and the utterances referenced to those topics i guess so the topics don't contain any redundant thing of like showing the whole topic again but they just sort of say a number and where they start and where they finish and the utterances then say which topic they belong to yeah no but i was thinking of the topic segmentation now and and f for that there would only be one right because it's sort of like it's just a time window yeah so if this lazy loading works then this should definitely fit into i mean not memory then because it wouldn't all be in memory at the same time so if we just have those sort of that information like a long list of all the utterances slash segments and like short or smaller lists which give weight to them and even though probably if there's a lot of over-head in having two different files we can probably merge the weights into it off-line you know what i mean like if if there's a lot of bureaucracy involved with having two different trees and whether one ties to the other because the one has the weight for the other then it's probably quicker to just yeah i thought that was the whole beauty that like you can just make a new x_m_l_ file and sort of tie that to the other and and it tre oh yeah so no i didn't i didn't mean tree no no i meant just like handling two different files internally sort of c i was just thinking you know like if if the overhead for having the same amount of data coming from two d files instead of from one file is massive then it would probably be for us easy to just like off-line put the the weight into into the file that has the segments uh yeah segments slash utterances already but that we can figure out i mean if it's going horrendously wrong yeah yeah yeah no we'd we'd be completely using like the whole infrastructure and basically just i mean the main difference really between our project and theirs really is that we load a different part of the data but otherwise we're doing it the same way that they are doing it so we just we're sort of running different types of queries on it we in a sense we i think we are running queries it's not just about um what we load and what we don't load but we're l running queries in the sense that we dynamically select by by weights don't we that we have to check how fast that is like to say give us all the ones that whether that works with their query language whether that's too many results and whether we shou you know if 'cause if it i let's say i mean if if their query language is strange and if it would return b ten million results and it can't handle it then we can just write our individual components in the way that they know which what the threshold is so they still get all the data and just they internally say oh no this is less than three and i'm not gonna display it or something hmm yeah no i'm just thinking for this whole thing of like a different level sort of cutting out different different pieces whether we do that through a query where we say give us everything that's ab above this and this weight or whether we skip the same infrastructure but every individual module like the player and the display say like they still get sort of all the different utterances uh all the different pieces but they say oh this piece i leave out because it's below the current threshold level when do we need the one for the meet okay yeah i guess for the so when we have the display will we display the whole series then if we have for the individual topic segments within the meetings if we have ready calculated disp um measures then we don't have to sort of extract that data from the individual utterances yeah and that's also fairly easy to store along with our segments isn't it for the segments are we extracting some type of title for them that we craft with some fancy algorithm or manually or we're just taking the single most highly valued key-word utterance for the segment heading hmm hmm it's probably like in in the end probably it wouldn't be the best thing if it's just the high most highly ranked phrase or key-word because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction or something yeah also like for this part maybe if we go over it with named entity in the end if i mean w if one of the people doing dil has some named entity code to spare and just like at least for the for sort of for finding topics titles for for segments just take a named entity which has a really high what's it called d_f_i_d_f_ whatever 'cause you'd probably be quite likely if they're talking about a conference or a person that that would be a named entity which is very highly fr um frequented in that part yeah he said they're quite sparse so that basically was don't bother basing too much of your general calculation on it but like especially if they're sparse probably individual named entities which describe what a what a segment is about would probably be quite good like if there's some name of some conference they would could probably say that name of the conference quite often even though he's right that they make indirect references to it anyway sorry so you're doing that on a on a per word level okay okay okay cool i was just wondering where you had the corpus from at the moment so it it seems that the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with that data structure in memory in the browser but it's just done off-line and everyone can ha represent it anyway they want as long as they sort of store it in a useful x_m_l_ representation in the end so like yeah that would mean understanding the nite x_m_l_ x_m_l_ sort of format in a lot more detail we should i think we should just have a long session in the computer room together and like now that we know a bit more what we want take a closer look at nite x_m_l_ mm-hmm mm-hmm good yeah i haven't looked at this stuff much at all yeah yeah who's who's sort of doing the the the central coordination of of of the browser application now like hmm yeah or but also like all these elements like like the loading and yeah integration and and like handling the data loading and stuff nah i'm sort of like i think i'll take over the display just because i've started with a bit and found it found it doable so somebody should sort of be the one person who's who understands most about what's t centrally going on with with the with the project like with the with the browser as a whole and where the data comes in and any volunteers it's also a complicated one yeah i know but uh b i guess we can do it like several people together it's probably just those people have to work together a lot and very closely and just make sure that they're always f understand what the other one is doing yeah or or ready-made versions of them for that matter and yeah but i think actually like at the moment the integration comes first i mean it's sort of at the moment the building the browser comes first and then only comes the creating new sophisticated data chunks because that's sort of the whole thing about having a prototype system which is more or less working on on chunk data but it at least we have the framework in which we can then test everything and and look at everything 'cause before we have that it's gonna be very difficult for anyone to really see how much the work that they're doing is making sense because you just well i guess you can see something from the data that you have in your individual x_m_l_ s files files that you create but it would be nice to have some basic system which just displays some stuff or just adapt like their like just sort of go from their system and and adapt that piece for piece and see how we could how we could arran like adapt it to our system does anyone want to like just sit with me and like play for three hours with nite x_m_l_ at some point uh i wouldn't like to be 'cause i'd like to go to the gym i'm theoretically free but if there's any time t hmm you have nothing no free time on wednesday hmm nine 'til twelve and then nothi you have or you hmm anytime wednesday afternoon i'd be cool i think yo forrest hill whatever one's easier to discuss stuff i don't know i'm not biased okay what time do you wanna do okay so i'll just meet you in in eighteen a in the afternoon i guess at the moment nobody critically depends on like the nite x_m_l_ stuff working right now right like at the moment you can all do your stuff and i can do my l_s_a_ stuff and i can even do the display to a vast degree without actually having their supplying framework working so it's not that crucial yeah actually i need the raw text as well yeah but i was i was i was more thinking of the sort of the the whole browser framework as a running programme now yeah i think we all need the raw text in different in different flavours don't we but number within the x_m_l_ context are they spoken numbers like do they look like they're utterances numbers there's the number task isn't there that's part of the whole thing hmm okay hmm yeah we have to probably cut that out anyway for our project i don't know it's probably gonna screw up a lot of our data otherwise if not sure if it what it does to document it would probably make the yeah if if you have segments for that probably the okay uh i'm just thinking like it pro it pro probably like the l_s_a_ would perform quite well on it it would probably find another number task quite easily seeing that it's a constrained vocabulary with a high co-occurrence of the same nine words so that wou ten word hmm yeah i think it's also something that they they said the numbers in order right yeah i think it's it the it sounded like they wanted to check out how well they were doing with overlapping and stuff because basically it's like they're reading them at different speeds but you know in which order they are said anyway icsi has some reasons for doing it they must have been pissed off saying like numbers at the end of every meeting um dave if you would or actually for well if you're doing i_d_f_s or you whatever you call your your frequencies i always mix up the name uh you need some dictionary for that at some point though like you need to have some representation of a word as not not that specific occurrence of that word token but of of of a given word form because you're making counts for word forms right yeah so we should work together on that because i need a dictionary as well okay 'kay okay didn't you say that the o the ord yeah but for i'm just wondering for the whole thing does somebody wo who was it of you two who said that um there's some programme which spits out a dictionary probably with frequencies okay is anyone of you for the for the document frequency over total frequency you gonna have total frequencies of words then with that right like over the whole corpus sort of or w using which tool are you talking about be careful with that like my experience with the british national corpus was that there's far more word types than you ever think because anything that's sort of unusual generally is a new word type like any typo or any strange thing where they put two words together and also any number as a word type of its own so you can easily end up with hundred thousands of words when you didn't expect them so generally dictionaries can grow bigger then you think they do well you can probably also you can probably pre-filter like with regular expressions even just say if it consists of only dig digits then skip it or even if it consists any special characters then skip it because it's probably something with a dot in between which is usually not something you wanna have and what i did for my project i just ignored the hundred most frequent words because they actually end up all being articles and and everything and stuff so we need like several of us need a dictionary am i the only one who needs it with frequencies am i the only one who needs it with frequencies or frequencies yeah well i guess as soon as we have the raw text we can probably just start with the java hash map and like just hash map over it and see how far we get i mean we can probably on a machine with a few hundred megabyte ram you can go quite far you can write it on beefy so even if it goes wrong and even if it has a million words be oh yeah burning it on a like we should be able to burn the whole corpus just the x_ hmm ah i see i asked support about that two days ago in the informatics building there oh sorry in in appleton tower five the ones closest t two machines closest to the support office so i presume oh wait i have the exact email i think he's talking about sort of the ones that yeah if you if you enter the big room in the right-hand corner i think um the thing is like you can only burn from the local file-system so if it's from s well actually i think if it's mounted you can directly burn from there but the problem is i have my data on beefy and so i have to get it into the local temp directory and burn it from there but you can burn it from there uh we looked that up and i for we looked that up and i forgot yeah yeah no you you we should be able to get it at i don't think it was i don't think it was a gigabyte hmm see i would off i would offer you to to get it on this one and then um like copy it but you know what i figured out i'm quicker down-loading over broad-band into my computer than using this hard disk there's something strange about the way how they access the hard disk how they mount it which is unfortunate hmm what operating system do you have okay wh what connection do you have at home yeah so if anyone of us gets it we can then just use an ext hmm yeah burn it to c_d_ or yeah put it on on hard disk whatever question is if you're not quicker if you uh because you should get massive compression out of that like fifty percent or something with a good algorithm so if you could compress it and just put it into a temp directory like the temp the temps usually have for gigabyte three or two the temps yeah i do like i mean there's not guarantee that anything stays there but overnight it'll stay and i think the temps usually have ah yeah but that would have to be the temp directory off the machine you can s_s_h_ into directory of s_s_h_ yeah they wou they'd they'd probably hate you for doing it but they'd probably they'd like you more if you s_s_h_ uh into another computer compress it there and then sort of copy it into the into the gateway machine they have um if you s_s_ hey you know if you if you s_s_h_ and they have this big warning about doing nothing at all in the gateway machine yeah to your home machine i haven't i haven't figured out how to tunnel through the gateway into another machine yet it's not it's not easy definitely that's why i end up sort of copying stuff into the temp directory at the gateway machine sorry if this is boring everybody else this is just details and how to get stuff home from what we can probably just look at that together when we're meeting i'm sorry mm-hmm well yeah as soon as somebody gives me the raw text of the whole thing i can probably just implement like a five line java hash table frequency dictionary builder and see oh did you not say frequencies f of words in the whole sorry did uh so you'd you yeah you'd have to count it yourself yeah oh you don't wanna have different counts for each chunk but just like sort of for for something from old chunks oh yeah no that's yeah so once i write an ar like w if i write like an algorithm which does a hash um table dictionary with frequency from a raw text then the raw text can be anything so how far are we g uh how f how far are you getting raw text out of it do you think okay well that's good because for the dictionary the order doesn't make a difference does it so yeah so um i'll get that from you and i'll write the hash table which goes over that and creates a dictionary file so for the dictionary is it okay if i do whatever word blank frequency or something just p could everybody sort of start from that i mean i guess we can yeah i i need frequency as well well i think we might have a lot in common what we calculate because i for my latent semantic analysis need like counts of words within a document uh within a a segment actually within a topic segment can i convert these probabilities back into frequencies okay oh so that's what f rainbow does because that's what l_s_a_ builds on like it builds a f a document by frequency matrix so i could probably get that even though but i already have i already have my code to build it up myself no don't bother i have my code already um yeah so dave you said you need the frequency counts actually for per document would you say not for the whole thing it more and more appears to me that if we if we scrap the notion of the meeting as an individual thing and sort of ju see meetings as as topic segments and have sort of like hierarchical topic segmentation instead then it's b like a more coherent framework wait are we are we using this um for the for the for the do for the weighting in the end now this this measure you're calculating because if we're doing like i think for for the information density we uh we should calculate it on the lowest level not on the highest but like 'cause yeah but w it don't you have to like go sort of like for in a document versus the whole thing isn't that how it works that you c look look at r i don't think that's a good idea because isn't it like that we expect th there to change over i b with the different topic segments more that they talk about something different in each different topic segment 'cause that's what relative term frequency is about that like in some context they're talking more about a certain word than in general so that would more be the the topic segments then i don't know yeah yeah yeah so i'm just wondering if there's ways to abandon the whole concept of of meetings and sort of but just not really treating separate meetings as too much of a separate entity but but on algorithmic level whether we actually whether there's some way to just represent meetings as as topics hmm that's not really what i meant but i think i have to think more about what i meant um g i'm confused about everything yeah i'm i'm not so concerned about the m a meeting plus something else i'm more talking about like yeah the keeping keeping the same algorithm and the same way of handling it and just saying like just this this topic here i uh it happens to be like a whole meeting and it has sort of sub-topics so just that sort of topics a hierarchical concept where like a topic where there can be super-topics and topics and the super-topics are in the end what the meetings are but in general at some level super-topics are treated like like topics hmm mm i'm not really sure what i want so sorry could describe that again the mm-hmm mm-hmm mm-hmm so that would be the series as a whole that would be sort of m meetings yeah yeah i'm a i'm a i'm a bit brain-damaged at the moment but i think i'll just sit together with you again and and go through it again hmm so so i'll is th it like is this and this structurally then always identical so that we can that we can treat it with the same algorithm or yeah i'm also not sure how we can go from from bottom-up i have always thought it's like more that oh whatever i'm a can't think of it at the moment probably this is all too complicated worrying about that at that moment anyway now have have we have we decided anything are we doing anything s wednesday we are meeting and looking at their at their implementation in some more detail to actually understand what's going on we had two things from their stuff just to make sure that we are like understand it we understand it enough to to m modify it yep how would we do that by just making like it w read write for everyone 'kay who has most free space on their same here well we alternatively we can probably just make another directory on the beefy scratch space i mean that's where i'm having gigabytes and gigabytes of stuff at the moment no no yeah but i think if he sends to the i think if he sends to the port he'd probably be in a better position yeah hmm i think he said yes to that i think uh that was like in when we were still in the seminar room i asked that once or like ask is it possible to get it off and nobody said like people were discussing about the technical practicalities but nobody said anything about al being allowed to or not allowed to i mean we have access to it here and i guess it probably means that we we can't give it to anybody else but but if they give us access to it here o sitting on a dice machine then there shouldn't be a reason why we shouldn't be able to use it on our laptop i personally don't have too many friends who would be too keen on getting it anyway i have that really excited pirate copied thing it annotated meeting data huh wait wait wait um sorry yeah sorry what i just realised we should really t keep different serieses completely separate for virtually all purposes just let's be careful about that because like the the icsi corpus isn't isn't one meeting series it's several meeting series with different people meeting for completely different things for each meeting alright okay but like let's just be careful that whatever we sort of we merge together that like the highest level of merging it's not the whole icsi corpus but individual series i think we might actually i think that's probably be somewhere like well or something like it um i think we might just get away with for the whole project just like looking at only one series and just doing within one series i mean you can do everything you want in one series oh yeah let's take that is the is the data always clearly split up by different series uh like is it easy to just pick one okay okay okay okay so at at every level everyone has to be careful to really just take even at the highest level just take stuff from one series and not merge stuff from different series together because they would probably be just majorly messy yeah so so t so like if even if we make one single text file which has the whole corpus sort of our corpus that would still be from one series only wou but it what you're producing at the moment is like individual text files that sort of have the raw text for a whole a meeting as a whole or mm-hmm yeah 'kay um so is is anybody creating an uh a real raw text thing at the moment like which is just the words yeah tha 'cause that's what i'm gonna need as well but i but if there uh b aren't like so it's it's start and end times just for the file like is it just the first and the last line or is it for every single thing in so what do you mean by just not print out that okay if you're into it can you make a text file which just like makes just the words 'kay do you want it straight flowing 'cause i would need something that marks the end of uh of uh is is yours segmented by topics then that like is there any information that you have to the topic to the automated topic topic segmentation oh then i need something different later anyway okay but for now if you c okay you're gonna put that as an output of yours the segmentation okay so for now can you create like sort of just uh a dump which is pure text just pure text so that i can get a dictionary and you can work on that for your topic segmentation and or for for the series but i can but i can also deal with separate files i mean i can just write the algorithm that it loads all files in a directory or something but i mean if you but if you can put it in one single mega-file that would be quite useful for me even though for you wouldn't it be easier if you had different files because then you sort of know like yeah so give m give me different files as long as like it m if you could name them in a way that is easy to enumerate over them like whatever one two three four five or something or just anything that i can yeah is is it something that's easily enu like to enumerate over is it some just some ordered pattern okay cool okay cool yeah in the right order it's just a wish list orders when do you think you'll have um like a primitive segmentation by some ready-made topic segmentation by some ready-made tool ready okay okay cool 'cause i'll need that then when it's done okay mm-hmm what's what's nine megabyte the the that sounds quite reasonable that's nine nine characters over okay okay okay that is for are we are we picking one particular series at the moment or yes okay yeah yeah i guess we can probably process the data for all different series and then check which series is the best for the presentation it sounds quite reasonable nine megabyte i mean if you think if it's r roughly a million words and nine characters per word sounds realisti yeah yes i'm gonna build a dictionary then from that like just a list of the words that maybe a list of the words with the frequencies or a list of the words sorted alphabetically or numerically what what does anyone want does this there any wishes for dictionaries so i'll create a dictionary add add the structure yeah and then the actual file we can probably like copy from your home directory or something like it yeah yeah but i'm sa i'm saying for the whole thing in the end then like the big thing we probably shouldn't do by email yeah oh from the time i get the file i can do that in an afternoon the next sort of the next morning oh you mean how long processing time it takes ah it's a it's a bog standard algorithm i've i've sort of i've written it for for dil just in half an hour or something similar it's just you put them in a hash table and and say well if it exists already in the hash table then you increase the count by one and i'll probably implement some filter for filtering out numbers or something really how do you do that okay well i don't know any perl i mean if anyone wants to do a perl script for that that does it does it nicely i uh i've no problem with that i but i think i have the java code virtually ready because for dil i wrote something very similar like for dil i wrote something that counts the the different occurrences of all the tags um sorry the hash table uh i've never serialized anything wouldn't that be absolutely massive though and then seriali and then write the serialization to a file so you want like a se like a file which is the serialization of a hash table okay yeah i i'll i'll check if i understand how it works i mean otherwise i can give you the code for loading a dictionary give you my my it's just it's it's sort of it's a line break separated file you know yeah yeah i'll see if i understand how to serialize there's a there's a serialise command so that gives me one mega mother of a s yeah but do they automatically write to the file anyway i'll i'll figure that out we don't have to yes is that pretty much pretty much it so dave and me look at how nite x_m_l_ works and we're hmm i'll build a dictionary as soon as i get the text and yeah so that when do we have to meet again then with this how are we gonna do a demonstrator next week my god no no not demonstrate but like didn't you say that uh didn't we sort of agree that it would be useful to have a demonstrator of it like some primitive thing working next week that's gotta be very prototype mm-hmm ah well let's go sorry i feel like like hanging mid-air and not really like finding a point where you can get your teeth into it and start working properly and so it's all so fuzzy the whole yeah but it at the moment but at the moment it's also an implementational level like with the data structures i'm just like over these vague ideas of some trees i'm f yeah it's just we are half-way through the project time table that's just what freaks me out um